{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wMQ_95i8QPZI"
   },
   "source": [
    "# **REINFORCEMENT LEARNING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tmSnmc0-QW8g"
   },
   "source": [
    "# **1. FONTES**\n",
    "\n",
    "- [Reinforcement Learning: With Open AI, TensorFlow and Keras Using Python - Abhishek Nandy e Manisha Biswas](https://www.amazon.com/Reinforcement-Learning-TensorFlow-Keras-Python-ebook/dp/B077ZZR4MZ)\n",
    "- [Reinforcement Learning Tutorial - edureka!](https://www.youtube.com/watch?v=LzaWrmKL1Z4)\n",
    "- [GitHub](https://github.com/keras-rl/keras-rl/blob/master/rl/agents/dqn.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y12s40imRvX-"
   },
   "source": [
    "# **2. O QUE É REINFORCEMENT LEARNING**\n",
    "Durante o processo de aprendizagem o algoritmo segue a ideia de tentativa e erro, em que ele aprende com seus erros. Por exemplo. Se nosso objeto de estudo for um carro que funciona com aprendizagem por reforço, esse carro vai bater em diversos lugares até que de tanto bater ele entende por onde tem que ir. Ou seja, o método é baseado em experiencia.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zJDLHj3eTwu_"
   },
   "source": [
    "# **3. COMO O ALGORITMO ENTENDE QUE ESTÁ INDO PELO CAMINHO CERTO**\n",
    "O processo de aprendizagem por reforço possui um agente e um ambiente. Sendo que o agente é o algoritmo que é quem está tentando aprender. E o ambiente seria o local que o agente precisa conhecer. \n",
    "\n",
    "Vejamos um exemplo: Suponha que temos uma criança aprendendo a andar, e foi estabelecido que essa criança teria que 10 metros. A criança seria o agente, e a casa dela seria o ambiente e os 10 metros seriam o objetivo. \n",
    "\n",
    "O método de aprendizado para essa criança andar 10 metros pode ser estabelcido por meio de recompensas, ou seja, quando a criança consegue cumprir o objetivo ela ganha algo, e quando não consegue ela recebe uma punição ou simplesmente não ganha nada, assim ela ganha estimulo a sempre cumprir o objetivo, afinal ela gosta da recompensa e não gosta da punição.\n",
    "\n",
    "Porém logo se vê que pedir para uma criança que está aprendendo a andar que ande logo de cara 10 metros, não é uma boa ideia. Sendo assim é mais prudente segmentarmos o objetivo da criança para 1 metro, depois 2, depois 3 e assim por diante, até que ela consiga andar os 10 metros.\n",
    "\n",
    "O processo de treinamento de um algoritmo com reforço é exatamente esse, o ambiente é reduzido para que o agente aprenda gradualmente e a cada acerto ele ganha um feedback positivo e a cada erro ele ganha um feedback negativo.\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/1600/1*vz3AN1mBUR2cr_jEG8s7Mg.png)\n",
    "Fonte: https://medium.freecodecamp.org/a-brief-introduction-to-reinforcement-learning-7799af5840db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uCSNVrdtYFoR"
   },
   "source": [
    "# **4. O QUE PODEMOS FAZER COM ESSES ALGORITMOS**\n",
    "Abaixo podemos ver um algoritmo de machine learning que por meio de treinamento por reforço consegue vencer o jogo de pacman facilmente. E diversos outros jogos também receberam boots treinados da mesma maneira. O projeto [DeepMind](https://deepmind.com/) por exemplo já conseguiu vencer os melhores jogadores do mundo em xadres, shogi e go utilizando o projeto [AlphaZero](https://deepmind.com/blog/alphazero-shedding-new-light-grand-games-chess-shogi-and-go/).\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/1600/1*D7JNcbvhP5UOR6_Ul-WJaw.gif)\n",
    "Fonte: http://ai.berkeley.edu/project_overview.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AtvfPvmVaKKb"
   },
   "source": [
    "# **5. PARTE TÉCNICA**\n",
    "\n",
    "- Recompença: Uma função que retornará uma recompença pela ação anterior.\n",
    "- Politica: A abordagem que o agente usará para tomar uma decisão com base na ação anterior.\n",
    "- Valor: O retorno experando a longo prazo, com descontos baseados nas recompensas a curto prazo.\n",
    "- Valor de ação: Cumpre a mesma função que `valor` porém leva em consideração a ação atual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M6FSI_9LhbfW"
   },
   "source": [
    "# **6. Q LEARNING**\n",
    "\n",
    "Vou explicar o algoritmo `Q Learning` por meio de um grafo de exemplo, sendo ele o grafo abaixo, em que queremos ir do ponto 2 ao ponto 5, e que cada rota (setas pretas) possuem uma recompensa, logo o nosso trajeto deve passar pela rota com a maior recompensa.\n",
    "\n",
    "![](https://github.com/LucasFDutra/Estudos-De-Machine-Learning/blob/master/018%20-%20Reinforcement%20Learning/Imagens/figura_1.jpg?raw=true)\n",
    "\n",
    "O algoritmo segue os seguintes passos:\n",
    "- 1 - Montar uma matriz `R` que represente o grafo. A matriz seguirá as regras abaixo.\n",
    "  - 0: Sem recompensa\n",
    "  - -1: Não possui rota\n",
    "  - x: Valor da recompensa\n",
    "- 2 - Inicializamos uma matriz `Q` com a mesma dimensão de `R` porém contendo apenas zeros.\n",
    "- 3 - Escolhe-se randomicamente um vertice x para partir.\n",
    "- 4 - Verificamos quais são os vertices y possíveis para ligar com x.\n",
    "- 5 - Verifica os vertices para os quais podemos ir após y.\n",
    "- 6 - Calculo de `Q(x, y)`. A equação para a atualização de `Q` é dada pela equação:\n",
    "  - $Q(x[i], y[i]) = R(x[i], y[i]) + gama . Max[Q(x[i+1], combinações)]$\n",
    "    - Como saimos de um ponto x para um ponto y, o valor de `x[i+1]` será igual a y.\n",
    "    - $ 0 <= gamma < 1$\n",
    "      - gamma = 0: significa que a recompensa está associada ao\n",
    "somente estado atual.\n",
    "     - gamma = 1: significa que a recompensa é de longo prazo.\n",
    "     - Olhando para a equção vemos que o gamma pode ser dito como um fator de consideração do valor futuro.\n",
    "- 7 - Ajuste Q\n",
    "- 8 - Volte para 3.\n",
    "\n",
    "**Explicando a montagem de R**\n",
    "\n",
    "Cada vertice tem sua linha e coluna. A linha representa o vertice que está ligando e a coluna o vertice que está sendo ligado. Ou seja, o elemento `R(1,3)` é a recompensa que temos indo de 1 para 3. No exemplo a matriz `R` é montada da seguinte forma:\n",
    "\n",
    "\\begin{array}{ccc}\n",
    "-1 & -1  & -1 & -1 & 0 & -1 \\\\ \n",
    "-1 & -1 & -1 & 0 & -1 & 100\\\\\n",
    "-1 & -1 & -1 & 0 & -1 & -1 \\\\ \n",
    "-1 & 0 & 0 & -1 & 0 & -1 \\\\\n",
    "0 & -1 & -1 & 0 & -1 & 100 \\\\\n",
    "-1 & 0 & -1 & -1 & 0 & 100 \\\\\n",
    "\\end{array}\n",
    "\n",
    "**Explicando a equação Q**\n",
    "\n",
    "- 1º) Como saimos de um ponto x e iremos para um ponto y, o proximo valor de x será o y anterior.\n",
    "- 2º) `Q(x[i+1], combinações)` são todos aqueles pontos em que na linha `x[i+1]` temos ligações válidas (valores >= 0). No caso do exemplo, se escolhermos partir de `R(1,5)`, vamos olhar para a linha 5 e veremos que os pontos que teremos valores >=0 são: `R(5,1), R(5,4) e R(5,5)`. Assim os valores que vamos utilizar para comparar quais são os maiores serão os valores nas posições `Q(5,1), Q(5,4) e Q(5,5)`\n",
    "\n",
    "\n",
    "**Rodando o exemplo uma vez**\n",
    "\n",
    "- 1 - A matriz R já foi mostrada anteriormente.\n",
    "- 2 - Inicializamos a matriz Q com zeros.\n",
    "\n",
    "\\begin{array}{ccc}\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "\\end{array}\n",
    "\n",
    "- 3 - Vamos dizer que o sorteio disse para iniciarmos em x=1.\n",
    "- 4 - Olhando a linha 1, vemos que podemos ir para 3 ou 5. Vamos dizer que o orteio entre 3 e 5 resultou em y=5. \n",
    "  - Ou seja vamos calcular `Q(1,5)`. E por consequência `x[i+1] = 5`\n",
    "- 5 - Agora vamos para a linha 5 e vemos que posteriormente poderemos ir para 1, 4, e 5.\n",
    "  - Vemos que temos as combinações `R(5,1), R(5,4) e R(5,5)`. Então para o calculo de Q iremos olhar para `Q(5,1), Q(5,4) e Q(5,5)`\n",
    "- 6 -  Temos que `Q(5,1)=0, Q(5,4)=0 e Q(5,5)=0`.  E definindo $gamma = 0.8$. Temos a equação:\n",
    "  - $Q(1, 5) = R(1, 5) + gama . Max[Q(5,1), Q(5,4) e Q(5,5)]$\n",
    "  - $Q(1, 5) = 100 + 0,8 . Max[0, 0, 0] = 100$\n",
    "- 7 - A nova matriz Q é igual à:\n",
    "\n",
    "\\begin{array}{ccc}\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 100 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "\\end{array}\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FtTy8EwHxza8"
   },
   "source": [
    "## 6.1 IMPLEMENTAÇÃO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tmfLnd3OyFkv"
   },
   "source": [
    "### Passo 1 - Definindo matriz R e parâmetro gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8ldwzMpOYFI3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "R = np.matrix([[-1, -1, -1, -1, 0, -1],\n",
    "               [-1, -1, -1, 0, -1, 100],\n",
    "               [-1, -1, -1, 0, -1, -1],\n",
    "               [-1, 0, 0, -1, 0, -1],\n",
    "               [-1, 0, 0, -1, -1, 100],\n",
    "               [-1, 0, -1, -1, 0, 100]])\n",
    "\n",
    "gamma = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9QCb4msEIuKF"
   },
   "source": [
    "### Passo 2 - Iniciando matriz Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WzZehWwSIlYP"
   },
   "outputs": [],
   "source": [
    "Q = np.matrix(np.zeros([6,6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZnqqUv9ZKHrr"
   },
   "source": [
    "### Passo 3 - Definindo um vertice x inicial entre 0 e 5\n",
    "\n",
    "Não é necessário fazermos isso agora, pois será feito durante o treinamento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "omd2ILY6yQe_"
   },
   "source": [
    "### Passo 4- Verificar vertices y que ligam com x\n",
    "\n",
    "Como vamos iniciar de um vertice, precisamos saber à quem esse vertice está ligado, a função abaixo verifica isso. Para tal ela verifica valores maiores ou iguais a zero, pois quando não existe ligação temos -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ctdkz5xrRvId"
   },
   "outputs": [],
   "source": [
    "def y_para_ligar_com_x(posicao_atual_em_x):\n",
    "  ligacoes_validas = np.where(R[posicao_atual_em_x] >= 0)[1] #onde obedecer a condição ele colocará num array val\n",
    "  y = int(np.random.choice(ligacoes_validas,1))\n",
    "  return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zg6dwF-43PlZ"
   },
   "source": [
    "### Passos 5, 6 e 7 - Calculando valor de Q e o atualiza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I3Z1KjfR01z8"
   },
   "outputs": [],
   "source": [
    "def updateQ(x, y, gamma):\n",
    "  max_index = np.where(Q[y,] == np.max(Q[y,]))[1] # verifica os maiores valores e retorna onde estão (indices)\n",
    "  \n",
    "  if max_index.shape[0] > 1: # quando tenho várias opções de maior valor\n",
    "    max_index = int(np.random.choice(max_index, size=1)) #seleciona algum dos maiores\n",
    "  else:\n",
    "    max_index = int(max_index)\n",
    "  \n",
    "  max_valor = Q[y, max_index] # retorna qual é o maior valor\n",
    "  \n",
    "  Q[x, y] = R[x,y] + gamma*max_valor # atualiza Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-fGatDc2004E"
   },
   "source": [
    "### Treinamento\n",
    "\n",
    "Vamos colocar o conjunto de funções criadas anteriormente para trabalhar e efetuar um treinamento de 10 mil interações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xrf5HDjN6aDQ"
   },
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "  \n",
    "  # 3 - Escolhe-se randomicamente um vertice x para partir.\n",
    "  x_atual = np.random.randint(0, int(Q.shape[0])) #valor aleatório de 0 a 5\n",
    "\n",
    "  # 4 - Verificamos quais são os vertices y possíveis para ligar com x.\n",
    "  y_atual = y_para_ligar_com_x(x_atual)\n",
    "  \n",
    "  # 5/6/7 - Calculo de `Q(x, y)` e ajustando Q\n",
    "  updateQ(x_atual, y_atual, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "aBX47ZKY7HGS",
    "outputId": "3ba316d3-980f-4813-de8c-507fb307fa25"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[  0.,   0.,   0.,   0., 400.,   0.],\n",
       "        [  0.,   0.,   0., 320.,   0., 500.],\n",
       "        [  0.,   0.,   0., 320.,   0.,   0.],\n",
       "        [  0., 400., 256.,   0., 400.,   0.],\n",
       "        [  0., 400., 256.,   0.,   0., 500.],\n",
       "        [  0., 400.,   0.,   0., 400., 500.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sqx3WMoY7RQp"
   },
   "source": [
    "### Verificando melhor caminho\n",
    "\n",
    "Vamos sair de 2 e ir para 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Vmvm8-IV7YMx",
    "outputId": "8f0f78b3-bb68-4419-fcab-65916d19f631"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o melhor caminho é: [2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "saida = 2\n",
    "chegada = 5\n",
    "\n",
    "passos = [saida]\n",
    "\n",
    "vertice_atual = saida\n",
    "\n",
    "while (vertice_atual != chegada):\n",
    "  next_step_index = np.where(Q[vertice_atual,] == np.max(Q[vertice_atual,]))[1]\n",
    "  \n",
    "  if next_step_index.shape[0]>1:\n",
    "    next_step_index = int(np.random.choice(next_step_index, size=1))\n",
    "  else:\n",
    "    next_step_index = int(next_step_index)\n",
    "    \n",
    "  passos.append(next_step_index)\n",
    "  vertice_atual = next_step_index \n",
    "  \n",
    "print(\"o melhor caminho é: {}\".format(passos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q6klOjaiA9qC"
   },
   "source": [
    "# **7. DEEP REINFORCEMENT LEARNING**\n",
    "\n",
    "Diferentemente do método tradicional de reinforcement learning, em que temos uma matriz Q para tormarmos decisões, aqui teremos uma rede neural que tomará essas decisões por nós. \n",
    "\n",
    "Esse tipo de treinamento é viável quenado temos ambientes muito grandes, visto que montarmos um grafo de recompensas e uma matriz Q e analizá-la se torna praticamente impossível em casos como jogos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q6klOjaiA9qC"
   },
   "source": [
    "## IMPLEMENTAÇÃO\n",
    "\n",
    "Mais abaixo você verá um exemplo de implementação do algoritmo Deep Q-learning. Aplicado ao jogo `CartPole`, que objetiva manter o pendulo reverso imóvel.\n",
    "\n",
    "![](https://camo.githubusercontent.com/86630851d533e6422d4c8cfe4ecccbec7fdf2e3f/687474703a2f2f61626973756c636f2e636f6d2f696d672f63617274706f6c652e676966)\n",
    "\n",
    "Fonte: https://github.com/anthonytec2/ssp-rl-final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eUwgstZ4BeEH"
   },
   "source": [
    "## Importantdo bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "eYh_9LUQA1cT",
    "outputId": "c9fd3891-b013-42b6-e7da-f64306adf31e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import gym\n",
    "import numpy as np\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.agents.dqn import DQNAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sIu_z0weBg0x"
   },
   "source": [
    "## Criando ambiente\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AFpR15ZEBgOx"
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G-uAW4NsBrRr"
   },
   "source": [
    "## Criando uma rede neural sequêncial\n",
    "\n",
    "- env.observation_space.shape: Retorna a dimensão do ambiente\n",
    "- env.action_space.n: Retorna a quantidade de ações que podemos tomar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401
    },
    "colab_type": "code",
    "id": "n22m7jwwBZN-",
    "outputId": "1cd6bc01-8522-4c0e-84fb-af959c733395"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 18)                90        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                304       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 700\n",
      "Trainable params: 700\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nb_actions = env.action_space.n\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.Flatten(input_shape=(1,)+env.observation_space.shape))\n",
    "model.add(keras.layers.Dense(18, activation='relu'))\n",
    "model.add(keras.layers.Dense(16, activation='relu'))\n",
    "model.add(keras.layers.Dense(16, activation='relu'))\n",
    "model.add(keras.layers.Dense(nb_actions, activation='linear'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WZ8GeW82AUz5"
   },
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit = 50000, window_length=1)\n",
    "policy = BoltzmannQPolicy()\n",
    "dqn = DQNAgent(model=model,\n",
    "               nb_actions=nb_actions,\n",
    "               memory=memory,\n",
    "               nb_steps_warmup=10, \n",
    "               target_model_update=1e-2,\n",
    "               policy=policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rSZ_sYIYG61K"
   },
   "source": [
    "## Preparando a rede neural para o treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z-OACUX8HTwj"
   },
   "outputs": [],
   "source": [
    "dqn.compile(optimizer = keras.optimizers.Adam(lr=1e-3), \n",
    "            metrics=['mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xjJSYuxFEa1b"
   },
   "source": [
    "## Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 5546
    },
    "colab_type": "code",
    "id": "oYRzNM0fEcq-",
    "outputId": "0c0cce9a-af72-4b36-f8a8-22d24b565c94",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/rl/memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f80d4725518>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env, nb_steps=50000, visualize = True, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja como o jogo se comporta durante o treinamento\n",
    "\n",
    "![](https://github.com/LucasFDutra/Estudos-De-Machine-Learning/blob/master/018%20-%20Reinforcement%20Learning/Imagens/figura_2.gif?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YyVF6IeyMri6"
   },
   "source": [
    "## Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dZVZB2R0Msyz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 200.000, steps: 200\n",
      "Episode 2: reward: 200.000, steps: 200\n",
      "Episode 3: reward: 200.000, steps: 200\n",
      "Episode 4: reward: 200.000, steps: 200\n",
      "Episode 5: reward: 200.000, steps: 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8128b8e4a8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja como o jogo se comporta depois de treinado\n",
    "\n",
    "![](https://github.com/LucasFDutra/Estudos-De-Machine-Learning/blob/master/018%20-%20Reinforcement%20Learning/Imagens/figura_3.gif?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **8. ENCONTRE O TESOURO**\n",
    "\n",
    "O algoritmo abaixo utiliza Q-learning, e tem o objetivo de que o `o` deve chegar ao T. Sendo que sempre que ele conseguir uma recompensa será dada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kw_2gsFD_QSc"
   },
   "source": [
    "## Importando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0KBFLp-q_QBT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vtj6FZCz_sFp"
   },
   "source": [
    "## Definindo mundo\n",
    "\n",
    "O nosso mundo terá um total de 10 posições até chegar no tesouro\n",
    "\n",
    "Gamma = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9GaDyFRV_PQS"
   },
   "outputs": [],
   "source": [
    "n_states = 6 # -----T\n",
    "actions = ['left', 'right']\n",
    "gamma = 0.8\n",
    "fresh_time = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NEXQMO50ABDE"
   },
   "source": [
    "## Construindo a matriz Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r6GCHMy8AAA5"
   },
   "outputs": [],
   "source": [
    "Q = np.matrix(np.zeros([n_states, len(actions)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n2dBXLWPEpN0"
   },
   "source": [
    "## Definindo a proxima ação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7-12sx3-Eota"
   },
   "outputs": [],
   "source": [
    "def choose_action(state, q_table):\n",
    "  q_line = q_table[state]\n",
    "  if (q_line == 0).all():\n",
    "    action = np.random.choice(actions)\n",
    "  else:\n",
    "    action = actions[np.argmax(q_line)]\n",
    "  return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VMidlfi4Ldvu"
   },
   "source": [
    "## Recebendo recompensa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z0E_2gsQKxTh"
   },
   "outputs": [],
   "source": [
    "def get_reward(state, action):\n",
    "  if (action == 'right'):\n",
    "    if (state == n_states-2): # está na posição 8 das 9, logo a proxima jogada terminará o jogo\n",
    "      reward = 1\n",
    "      new_state = 'T'\n",
    "    else:\n",
    "      reward = 0\n",
    "      new_state = state+1 # andou um para a direita\n",
    "  else:\n",
    "    reward = 0\n",
    "    if (state == 0):\n",
    "      new_state = state\n",
    "    else:\n",
    "      new_state = state-1\n",
    "  return new_state, reward     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fl_72aWxNE3L"
   },
   "source": [
    "## Atualizando ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jwJesezsNH3y"
   },
   "outputs": [],
   "source": [
    "def update_env(state, steps, attempt):\n",
    "  env = ['-']*(n_states-1)+['T'] # exibe '---------T'\n",
    "  if (state == 'T'):\n",
    "    print(f'\\r       tentativa: {attempt+1}, número de passos: {steps}',end='')\n",
    "    time.sleep(2)\n",
    "  else:\n",
    "    env[state] = 'o'\n",
    "    env_j = ''.join(env)\n",
    "    print(f'\\r{env_j}',end='')\n",
    "    time.sleep(fresh_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vblfZ9eiOyAb"
   },
   "source": [
    "## Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8P5admVxOpRt",
    "outputId": "cb94dc62-4d5d-449a-fa26-c5a502595004"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       tentativa: 10, número de passos: 4"
     ]
    }
   ],
   "source": [
    "for attempt in range(10):\n",
    "  state = 0\n",
    "  step = 0\n",
    "  update_env(state, step, attempt)\n",
    "  while (state != 'T'):\n",
    "    action = choose_action(state, Q)\n",
    "    new_state, reward = get_reward(state, action)\n",
    "    if (new_state != 'T'):\n",
    "      Q[state, actions.index(action)] = reward + (gamma * np.max(Q[new_state,]))\n",
    "    else:\n",
    "      Q[state, actions.index(action)] = reward\n",
    "    state = new_state\n",
    "    update_env(state, step, attempt)\n",
    "    step+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja como o jogo se comporta durante o treinamento\n",
    "\n",
    "![](https://github.com/LucasFDutra/Estudos-De-Machine-Learning/blob/master/018%20-%20Reinforcement%20Learning/Imagens/figura_4.gif?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **9. ENCONTRE A SAÍDA**\n",
    "\n",
    "Nesse algoritmo temos uma pequena representação de um labirinto. Sendo que a peça vermelha precisa encontar o circulo amarelo para sair do labrinto, sem bater nas peças pretas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEFININDO O MUNDO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/tkinter/__init__.py\", line 1705, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"/usr/lib/python3.6/tkinter/__init__.py\", line 749, in callit\n",
      "    func(*args)\n",
      "  File \"<ipython-input-22-c789f785cca7>\", line 127, in update\n",
      "    s, r, done = env.step(a)\n",
      "  File \"<ipython-input-22-c789f785cca7>\", line 82, in step\n",
      "    s = self.canvas.coords(self.rect)\n",
      "  File \"/usr/lib/python3.6/tkinter/__init__.py\", line 2469, in coords\n",
      "    self.tk.call((self._w, 'coords') + args))]\n",
      "_tkinter.TclError: invalid command name \".!canvas\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "if sys.version_info.major == 2:\n",
    "    import Tkinter as tk\n",
    "else:\n",
    "    import tkinter as tk\n",
    "\n",
    "\n",
    "UNIT = 40   # pixels\n",
    "MAZE_H = 6  # grid height\n",
    "MAZE_W = 6  # grid width\n",
    "\n",
    "\n",
    "class Maze(tk.Tk, object):\n",
    "    def __init__(self):\n",
    "        super(Maze, self).__init__()\n",
    "        self.action_space = ['u', 'd', 'l', 'r']\n",
    "        self.n_actions = len(self.action_space)\n",
    "        self.title('maze')\n",
    "        self.geometry('{0}x{1}'.format(MAZE_H * UNIT, MAZE_H * UNIT))\n",
    "        self._build_maze()\n",
    "\n",
    "    def _build_maze(self):\n",
    "        self.canvas = tk.Canvas(self, bg='white',\n",
    "                           height=MAZE_H * UNIT,\n",
    "                           width=MAZE_W * UNIT)\n",
    "\n",
    "        # create grids\n",
    "        for c in range(0, MAZE_W * UNIT, UNIT):\n",
    "            x0, y0, x1, y1 = c, 0, c, MAZE_H * UNIT\n",
    "            self.canvas.create_line(x0, y0, x1, y1)\n",
    "        for r in range(0, MAZE_H * UNIT, UNIT):\n",
    "            x0, y0, x1, y1 = 0, r, MAZE_W * UNIT, r\n",
    "            self.canvas.create_line(x0, y0, x1, y1)\n",
    "\n",
    "        # create origin\n",
    "        origin = np.array([20, 20])\n",
    "\n",
    "        # hell\n",
    "        hell1_center = origin + np.array([UNIT * 2, UNIT])\n",
    "        self.hell1 = self.canvas.create_rectangle(\n",
    "            hell1_center[0] - 15, hell1_center[1] - 15,\n",
    "            hell1_center[0] + 15, hell1_center[1] + 15,\n",
    "            fill='black')\n",
    "        # hell\n",
    "        hell2_center = origin + np.array([UNIT, UNIT * 2])\n",
    "        self.hell2 = self.canvas.create_rectangle(\n",
    "            hell2_center[0] - 15, hell2_center[1] - 15,\n",
    "            hell2_center[0] + 15, hell2_center[1] + 15,\n",
    "            fill='black')\n",
    "\n",
    "        # create oval\n",
    "        oval_center = origin + UNIT * 2\n",
    "        self.oval = self.canvas.create_oval(\n",
    "            oval_center[0] - 15, oval_center[1] - 15,\n",
    "            oval_center[0] + 15, oval_center[1] + 15,\n",
    "            fill='yellow')\n",
    "\n",
    "        # create red rect\n",
    "        self.rect = self.canvas.create_rectangle(\n",
    "            origin[0] - 15, origin[1] - 15,\n",
    "            origin[0] + 15, origin[1] + 15,\n",
    "            fill='red')\n",
    "\n",
    "        # pack all\n",
    "        self.canvas.pack()\n",
    "\n",
    "    def reset(self):\n",
    "        self.update()\n",
    "        time.sleep(0.5)\n",
    "        self.canvas.delete(self.rect)\n",
    "        origin = np.array([20, 20])\n",
    "        self.rect = self.canvas.create_rectangle(\n",
    "            origin[0] - 15, origin[1] - 15,\n",
    "            origin[0] + 15, origin[1] + 15,\n",
    "            fill='red')\n",
    "        # return observation\n",
    "        return self.canvas.coords(self.rect)\n",
    "\n",
    "    def step(self, action):\n",
    "        s = self.canvas.coords(self.rect)\n",
    "        base_action = np.array([0, 0])\n",
    "        if action == 0:   # up\n",
    "            if s[1] > UNIT:\n",
    "                base_action[1] -= UNIT\n",
    "        elif action == 1:   # down\n",
    "            if s[1] < (MAZE_H - 1) * UNIT:\n",
    "                base_action[1] += UNIT\n",
    "        elif action == 2:   # right\n",
    "            if s[0] < (MAZE_W - 1) * UNIT:\n",
    "                base_action[0] += UNIT\n",
    "        elif action == 3:   # left\n",
    "            if s[0] > UNIT:\n",
    "                base_action[0] -= UNIT\n",
    "\n",
    "        self.canvas.move(self.rect, base_action[0], base_action[1])  # move agent\n",
    "\n",
    "        s_ = self.canvas.coords(self.rect)  # next state\n",
    "\n",
    "        # reward function\n",
    "        if s_ == self.canvas.coords(self.oval):\n",
    "            reward = 1\n",
    "            done = True\n",
    "            s_ = 'terminal'\n",
    "        elif s_ in [self.canvas.coords(self.hell1), self.canvas.coords(self.hell2)]:\n",
    "            reward = -1\n",
    "            done = True\n",
    "            s_ = 'terminal'\n",
    "        else:\n",
    "            reward = 0\n",
    "            done = False\n",
    "\n",
    "        return s_, reward, done\n",
    "\n",
    "    def render(self):\n",
    "        time.sleep(0.1)\n",
    "        self.update()\n",
    "\n",
    "\n",
    "def update():\n",
    "    for t in range(10):\n",
    "        s = env.reset()\n",
    "        while True:\n",
    "            env.render()\n",
    "            a = 1\n",
    "            s, r, done = env.step(a)\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env = Maze()\n",
    "    env.after(100, update)\n",
    "    env.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja o nosso labirinto.\n",
    "\n",
    "![](https://github.com/LucasFDutra/Estudos-De-Machine-Learning/blob/master/018%20-%20Reinforcement%20Learning/Imagens/figura_5.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MANIPULAÇÃO DA TABELA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class QLearningTable:\n",
    "    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):\n",
    "        self.actions = actions  # a list\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.epsilon = e_greedy\n",
    "        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        self.check_state_exist(observation)\n",
    "        # action selection\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            # choose best action\n",
    "            state_action = self.q_table.loc[observation, :]\n",
    "            # some actions may have the same value, randomly choose on in these actions\n",
    "            action = np.random.choice(state_action[state_action == np.max(state_action)].index)\n",
    "        else:\n",
    "            # choose random action\n",
    "            action = np.random.choice(self.actions)\n",
    "        return action\n",
    "\n",
    "    def learn(self, s, a, r, s_):\n",
    "        self.check_state_exist(s_)\n",
    "        q_predict = self.q_table.loc[s, a]\n",
    "        if s_ != 'terminal':\n",
    "            q_target = r + self.gamma * self.q_table.loc[s_, :].max()  # next state is not terminal\n",
    "        else:\n",
    "            q_target = r  # next state is terminal\n",
    "        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)  # update\n",
    "\n",
    "    def check_state_exist(self, state):\n",
    "        if state not in self.q_table.index:\n",
    "            # append new state to q table\n",
    "            self.q_table = self.q_table.append(\n",
    "                pd.Series(\n",
    "                    [0]*len(self.actions),\n",
    "                    index=self.q_table.columns,\n",
    "                    name=state,\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CÓDIGO PRINCIPAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game over\n"
     ]
    }
   ],
   "source": [
    "def update():\n",
    "    for episode in range(100):\n",
    "        # initial observation\n",
    "        observation = env.reset()\n",
    "\n",
    "        while True:\n",
    "            # fresh env\n",
    "            env.render()\n",
    "\n",
    "            # RL choose action based on observation\n",
    "            action = RL.choose_action(str(observation))\n",
    "\n",
    "            # RL take action and get next observation and reward\n",
    "            observation_, reward, done = env.step(action)\n",
    "\n",
    "            # RL learn from this transition\n",
    "            RL.learn(str(observation), action, reward, str(observation_))\n",
    "\n",
    "            # swap observation\n",
    "            observation = observation_\n",
    "\n",
    "            # break while loop when end of this episode\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "    # end of game\n",
    "    print('game over')\n",
    "    env.destroy()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = Maze()\n",
    "    RL = QLearningTable(actions=list(range(env.n_actions)))\n",
    "\n",
    "    env.after(100, update)\n",
    "    env.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja como o jogo se comporta durante o treinamento\n",
    "\n",
    "![](https://github.com/LucasFDutra/Estudos-De-Machine-Learning/blob/master/018%20-%20Reinforcement%20Learning/Imagens/figura_6.gif?raw=true)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "Aprendizagem por reforço.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
