{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5a70062368a7086f7ed3d571de5f8e85453d05c0"
   },
   "source": [
    "# **REDUÇÃO DE DIMENSIONALIDADE**\n",
    "\n",
    "Como já deve ter notado até aqui, utilizar uma base de dados excessivamente grande pode gerar tanto gasto computacional que o algoritmo demora muito tempo para rodar, ou nem roda, como no caso do agrupamento hierárquico que precisei cortar uma parte da base de dados. Porém para efetuar esse corte existem técnicas que podem ser utilizadas para que sejam preservados aqueles atributos que vão melhor nos atender. Ou seja, atributos com maior representatividade para nosso problema.\n",
    "\n",
    "Antes de iniciar os estudos em cima dos algoritmos é necessário identificar uma diferença entre seleção de características e extração de características.\n",
    "\n",
    "- Seleção de características: O algoritmo identifica os atributos mais relevantes e utiliza apenas eles.\n",
    "- Extração de características: O algoritmo cria novos atributos que são a união de atributos parecidos, e os usa.\n",
    "\n",
    "## ANÁLISE DE COMPONENTES PRINCIPAIS (PCA)\n",
    "\n",
    "- Para dados linearmente separáveis.\n",
    "- Seleção de características.\n",
    "- O PCA é um algoritmo de aprendizagem não supervisionada.\n",
    "- Identifica a correlação entre as variáveis, e caso haja uma forte correlação é possível reduzir a dimensionalidade.\n",
    "- Das m variáveis independentes, PCA extrai `p <= m` novas variáveis independentes que explica melhor a variação na base de dados, sem considerar a variável dependente.\n",
    "    - Independente: previsores.\n",
    "    - Dependente: classe.\n",
    "- O usuário pode escolher o número de `p`.\n",
    "\n",
    "### **Implementação**\n",
    "- **Classe**:\n",
    "    - [sklearn.decomposition.PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)\n",
    "- **Base de Dados**:\n",
    "    - census.csv\n",
    "    \n",
    "Primeiramente vamos efetuar todo o processo de preprocessamento dos dados, pois ao longo desse material eles já sofreram alterações. Sendo assim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "d0aebb1e3e7d46c300d195b2ccf2eb69876900ee"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#----------------Coloca a base de dados na variável 'base'--------------------#\n",
    "base = pd.read_csv('../input/census.csv')\n",
    "\n",
    "#----------Alocando os valores das colunas em previsores e classes------------#\n",
    "previsores = base.iloc[:,0:14].values\n",
    "classe = base.iloc[:,14].values\n",
    "\n",
    "#------Definindo um objeto LabelEncoder que é responsável pela conversão------#\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "\n",
    "#-------Verificando onde estão as variáveis categóricas e as convertendo------#\n",
    "lista_categoricos = []\n",
    "for i in range(len(previsores[0])):\n",
    "    if type(previsores[0][i]) == str:\n",
    "        lista_categoricos.append(i)\n",
    "classe = labelencoder.fit_transform(classe)\n",
    "\n",
    "#---------------------Criando as variáveis do tipo dummy----------------------#\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct = ColumnTransformer([('oh_enc', OneHotEncoder(sparse=False), lista_categoricos),],remainder='passthrough')\n",
    "previsores = ct.fit_transform(previsores)\n",
    "\n",
    "#--------------------------Escalonando as variáveis---------------------------#\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "previsores = scaler.fit_transform(previsores)\n",
    "\n",
    "#-------------------Dividindo os Previsores e as Classes----------------------#\n",
    "# Obs.: test_size = <porcentagem da divisão dos dados>                        #\n",
    "#       nesse caso 25% dos dados serão de teste.                              #\n",
    "#-----------------------------------------------------------------------------#\n",
    "from sklearn.model_selection import train_test_split\n",
    "previsores_treinamento, previsores_teste, classe_treinamento, classe_teste = train_test_split(previsores, classe, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "784886d7ec55823b6d424c8e4a76bafbbec3b9bb"
   },
   "source": [
    "Agora sim podemos iniciar a utilização dos algoritmos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "757ba885dc78a738a37c40ebfe06f0402f759e3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04210413 0.02788386 0.02408172 0.02244915 0.02145237 0.01764136]\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------Biblioteca------------------------------------#\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#------Definindo a quantidade de atributos que quero que sobrem no final------#\n",
    "pca = PCA(n_components = 6)\n",
    "\n",
    "#---------Ajustando a quantidade de atributos para treinamento e teste--------#\n",
    "previsores_treinamento = pca.fit_transform(previsores_treinamento)\n",
    "previsores_teste = pca.transform(previsores_teste)\n",
    "\n",
    "#-Mostrando o quanto de representatividade vamos ter com esses p componentes--#\n",
    "componentes = pca.explained_variance_ratio_\n",
    "print(componentes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "80484aa5ec5fbe3254b7c826d8cb14b199059af6"
   },
   "source": [
    "Note que a base de dados foi reduzida para apenas seis atributos, isso porque elas já representavam bem toda a base de dados. Para descobrir quantos componentes devem persistir após os cortes, utiliza-se o comando `componentes = pca.explained_variance_ratio_ ` com ele a variável componentes terá como resultado uma lista que pode ser melhor vista da seguinte forma:\n",
    "\n",
    "|Quantidade|Representatividade individual|\n",
    "|-|-|\n",
    "|1|0.2705458|\n",
    "|2|0.17917156|\n",
    "|3|0.1547414|\n",
    "|4|0.14424214|\n",
    "|5|0.1378468|\n",
    "|6|0.1134523|\n",
    "\n",
    "Pode-se ler essa tabela da seguinte forma: se utilizarmos apenas um atributo teremos 27% de representatividade, se utilizarmos dois atributos teremos $(0,27+0,17).100$ % de representatividade. E assim sucessivamente. Logo quanto um dado valor de `n` atingir uma representatividade satisfatório é esse valor que será escolhido.\n",
    "\n",
    "\n",
    "Após a escolha do valor de atributos e da redução da base de dados, escolha o método de classificação ou regressão que lhe interessar e o aplique na sua nova base de dados.\n",
    "Para exemplificar vou utilizar o algoritmo de classificação Random forest, que dentre todos foi um dos que apresentou melhores resultados, cerca de 85%, com a base de dados census.csv completa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "09c4ca949de83d8d9c6cef9012f0a5d200e9a3cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precisao: 0.8272939442328953 \n",
      " matriz de confusão: \n",
      "[[5635  524]\n",
      " [ 882 1100]]\n"
     ]
    }
   ],
   "source": [
    "#------Criando a floresta com 40 árvores baseando no criterio de entropia-----#\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classificador = RandomForestClassifier(n_estimators = 40, criterion = 'entropy', random_state = 0)\n",
    "classificador.fit(previsores_treinamento, classe_treinamento)\n",
    "\n",
    "#------Passando os dados de teste palas árvores e armazenando as previsões----#\n",
    "previsoes = classificador.predict(previsores_teste)\n",
    "\n",
    "#----------------Exibindo a precisão e a matriz de confusão-------------------#\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "precisao = accuracy_score(classe_teste, previsoes)\n",
    "matriz = confusion_matrix(classe_teste, previsoes)\n",
    "print('precisao: {}'.format(precisao),'\\n','matriz de confusão: \\n{}' .format(matriz))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "088ac09e298b7f0c97bd9519efe894af62dee09c"
   },
   "source": [
    "## ANÁLISE DISCRIMINANTE LINEAR (LDA)\n",
    "\n",
    "- Para dados linearmente separáveis.\n",
    "- Extração de características.\n",
    "- Além de encontrar os componentes principais, LDA também encontra os eixos que maximizam a separação entre múltiplas classes.\n",
    "- Aprendizagem supervisionada.\n",
    "- Das `m` variáveis independentes, LDA extrai `p<=m` novas variáveis independentes que separam as classes da variável dependente.\n",
    "- Uma desvantagem do LDA é que como ele faz a redução com base nos previsores e na classe, se pedirmos para ele reduzir ele irá reduzir com base na dimensão das duas, ou seja, se a classe tiver apenas um atributo ele reduzirá tudo para apenas um atributo mesmo que mandemos reduzir para seis. Isso gera muita perda de informação.\n",
    "\n",
    "### **Implementação**\n",
    "- **Classe**:\n",
    "    - [sklearn.decomposition.LatentDirichletAllocation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html#sklearn.decomposition.LatentDirichletAllocation)\n",
    "- **Base de Dados**:\n",
    "    - census.csv\n",
    "    \n",
    "Novamente utilizarei o algoritmo de classificação Random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "00b0ed173f665ff5ea7b04ce952b31cf19441e75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precisao: 0.7580149858739712 \n",
      " matriz de confusão: \n",
      "[[5189  970]\n",
      " [1000  982]]\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------Biblioteca------------------------------------#\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "#------Definindo a quantidade de atributos que quero que sobrem no final------#\n",
    "lda = LinearDiscriminantAnalysis(n_components = 2)\n",
    "\n",
    "#---------Ajustando a quantidade de atributos para treinamento e teste--------#\n",
    "previsores_treinamento = lda.fit_transform(previsores_treinamento, classe_treinamento)\n",
    "previsores_teste = lda.transform(previsores_teste)\n",
    "\n",
    "#------Criando a floresta com 40 árvores baseando no criterio de entropia-----#\n",
    "classificador = RandomForestClassifier(n_estimators = 40, criterion = 'entropy', random_state = 0)\n",
    "classificador.fit(previsores_treinamento, classe_treinamento)\n",
    "\n",
    "#------Passando os dados de teste palas árvores e armazenando as previsões----#\n",
    "previsoes = classificador.predict(previsores_teste)\n",
    "\n",
    "#----------------Exibindo a precisão e a matriz de confusão-------------------#\n",
    "precisao = accuracy_score(classe_teste, previsoes)\n",
    "matriz = confusion_matrix(classe_teste, previsoes)\n",
    "print('precisao: {}'.format(precisao),'\\n','matriz de confusão: \\n{}' .format(matriz))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4dce5f544d63c1e16aaccc5a989b67af7b33212f"
   },
   "source": [
    "## KERNEL PCA\n",
    "\n",
    "- Para dados não linearmente separáveis.\n",
    "- Kernel PCA é uma versão do PCA que os dados são mapeados para uma dimensão maior usando o kernel trick.\n",
    "- Os componentes principais são extraídos dos dados com dimensionalidade maior.\n",
    "\n",
    "### **Implementação**\n",
    "\n",
    "- **Classe**:\n",
    "    - [sklearn.decomposition.KernelPCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html#sklearn.decomposition.KernelPCA)\n",
    "- **Base de Dados**:\n",
    "    - census.csv\n",
    "    \n",
    "Novamente utilizarei o algoritmo de classificação Random forest.\n",
    "\n",
    "Apesar de poupar o gasto computacional na hora do treinamento, ele gasta muito processamento para reduzir os atributos, e no caso dessa base de dados ele gasta principalmente memória."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "4dce5f544d63c1e16aaccc5a989b67af7b33212f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precisao: 0.7585063260041764 \n",
      " matriz de confusão: \n",
      "[[5195  964]\n",
      " [1002  980]]\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------Biblioteca------------------------------------#\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "#------Definindo a quantidade de atributos que quero que sobrem no final------#\n",
    "kpca = KernelPCA(n_components = 6, kernel = 'rbf')\n",
    "\n",
    "#---------Ajustando a quantidade de atributos para treinamento e teste--------#\n",
    "previsores_treinamento = kpca.fit_transform(previsores_treinamento)\n",
    "previsores_teste = kpca.transform(previsores_teste)\n",
    "\n",
    "#------Criando a floresta com 40 árvores baseando no criterio de entropia-----#\n",
    "classificador = RandomForestClassifier(n_estimators = 40, criterion = 'entropy', random_state = 0)\n",
    "classificador.fit(previsores_treinamento, classe_treinamento)\n",
    "\n",
    "#------Passando os dados de teste palas árvores e armazenando as previsões----#\n",
    "previsoes = classificador.predict(previsores_teste)\n",
    "\n",
    "#----------------Exibindo a precisão e a matriz de confusão-------------------#\n",
    "precisao = accuracy_score(classe_teste, previsoes)\n",
    "matriz = confusion_matrix(classe_teste, previsoes)\n",
    "print('precisao: {}'.format(precisao),'\\n','matriz de confusão: \\n{}' .format(matriz))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4dce5f544d63c1e16aaccc5a989b67af7b33212f"
   },
   "source": [
    "Mas uma observação válida de ser feita é que os resultados obtidos reduzindo a quantidade de atributos foram relativamente próximos do valor sem redução, logo o programador deve analisar se vale a pena ou não o gasto computacional de se utilizar todos os dados."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
